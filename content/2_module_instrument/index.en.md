---
title: "Module 2: Configure Observability"
weight: 40
---

In this module, you'll explore the OpenTelemetry instrumentation that's already built into the GenAI application and configure it to send telemetry data to Honeycomb. This demonstrates a **production-ready pattern** where observability is a first-class citizen from day one, not bolted on later.

## Module Overview

**Duration:** 15 minutes

**Objectives:**
- Understand the existing OpenTelemetry instrumentation architecture
- Review auto-instrumentation and custom tracing for GenAI workloads
- Verify Honeycomb configuration in the deployed application
- Generate traffic and confirm telemetry is flowing to Honeycomb
- Explore the trace structure and GenAI-specific attributes

## Key Concept: Observability From Day One

Unlike traditional approaches where instrumentation is added after deployment, this application demonstrates **observability-first architecture**:

âœ… OpenTelemetry instrumentation exists **before** the first deployment
âœ… Traces, logs, and metrics are **built into** the application architecture
âœ… Configuration is **declarative** via environment variables
âœ… GenAI-specific semantics follow **emerging standards**

::alert[**Philosophy**: In production systems, observability should be non-negotiable infrastructure, just like logging and error handling. This workshop shows how to build it in from the start.]{type="info"}

## Step 1: Explore the Existing Instrumentation

The ai-workshop application already has complete OpenTelemetry instrumentation. Let's examine how it works.

1. **Navigate to the server directory**:
   ```bash
   cd /workshop/ai-workshop/server
   ls -la
   ```

   Key files:
   - `instrumentation.js` - Entry point for tracing
   - `config/tracing.js` - OpenTelemetry SDK configuration
   - `utils/llmTracing.js` - Custom LLM tracing utilities
   - `index.js` - Server entry point (imports instrumentation FIRST)

2. **Examine the instrumentation entry point**:
   ```bash
   cat instrumentation.js
   ```

   ```javascript
   /**
    * OpenTelemetry Instrumentation Entry Point
    *
    * This file MUST be imported before any other application code
    * to ensure proper auto-instrumentation of libraries.
    */
   import { initializeTracing } from './config/tracing.js';

   // Initialize tracing immediately
   initializeTracing();
   ```

   **Key Insight**: Instrumentation is initialized **before** any other imports. This ensures auto-instrumentation captures all operations.

3. **Best Practice - High-Cardinality Attributes**:

   When instrumenting your application, Honeycomb excels at handling **high-cardinality data**â€”attributes with many unique values that traditional monitoring tools struggle with. The application includes attributes like:

   - `user.id` or `session.id` - Unique per user/session
   - `request.id` - Unique per request
   - `gen_ai.request.model` - Specific model version used
   - `deployment.environment` - Environment identifier

   ::alert[**Honeycomb Advantage**: Unlike traditional metrics systems that require pre-aggregation, Honeycomb stores raw event data with all high-cardinality attributes intact. This allows you to ask questions like "Show me all requests from user X where token usage exceeded Y" without pre-defining these queries.]{type="success"}

4. **Review the tracing configuration**:
   ```bash
   cat config/tracing.js | head -60
   ```

   This file configures:
   - **OTLP Exporters** for traces and logs
   - **Resource attributes** (service name, version, environment)
   - **Auto-instrumentations** for Express, HTTP, AWS SDK
   - **Winston instrumentation** for log correlation
   - **Honeycomb-specific configuration**

5. **Examine custom LLM tracing utilities**:
   ```bash
   cat utils/llmTracing.js | head -95
   ```

   The `traceLLMCall()` function wraps Bedrock API calls with:
   - **GenAI semantic conventions v1.0**: `gen_ai.system`, `gen_ai.operation.name`, `gen_ai.request.model`
   - **Token usage tracking**: `gen_ai.usage.input_tokens`, `gen_ai.usage.output_tokens`
   - **Error handling**: Proper span status, exception recording, and `error.type` attribute
   - **Response metadata**: `gen_ai.response.id`, `gen_ai.response.finish_reasons`

::alert[**Production Pattern**: Custom instrumentation captures business-specific metrics (token usage, model performance) that auto-instrumentation can't infer. This is critical for cost monitoring and SLO tracking.]{type="success"}

## Step 2: Understand Auto-Instrumentation

The application uses `getNodeAutoInstrumentations()` to automatically instrument popular libraries:

**What's instrumented automatically:**
- âœ… **Express.js**: HTTP request/response spans with route information
- âœ… **HTTP/HTTPS**: Outbound HTTP client calls (to Bedrock, OpenSearch)
- âœ… **AWS SDK**: All AWS service calls (Bedrock InvokeModel, OpenSearch queries)
- âœ… **Winston**: Log correlation with trace and span IDs

**Example trace structure** you'll see:
```
POST /api/chat (Express auto-instrumentation)
â”œâ”€ gen_ai.bedrock.chat (Custom GenAI instrumentation)
â”‚  â””â”€ AWS Bedrock InvokeModel (AWS SDK auto-instrumentation)
â”œâ”€ db.vector.search (Custom instrumentation - future enhancement)
â”‚  â””â”€ HTTPS POST OpenSearch (HTTP auto-instrumentation)
â””â”€ Response
```

Each span automatically includes:
- **Timing**: Duration, start/end timestamps
- **Context**: Trace ID, span ID, parent span ID
- **Attributes**: HTTP method, status code, URL, etc.
- **Resource**: Service name, version, deployment environment

## Step 3: Verify Honeycomb Configuration

The Pulumi infrastructure already configured the ECS task with Honeycomb environment variables. Let's verify:

1. **Check the ECS task definition**:
   ```bash
   cd /workshop/ai-workshop/pulumi
   pulumi env run honeycomb-pulumi-workshop/ws -i -- aws ecs describe-task-definition \
     --task-definition $(pulumi stack output ecsTaskDefinitionArn | cut -d'/' -f2) \
     --query 'taskDefinition.containerDefinitions[0].environment[?name==`OTEL_SERVICE_NAME` || name==`OTEL_EXPORTER_OTLP_ENDPOINT` || name==`HONEYCOMB_DATASET`]'
   ```

   Expected output:
   ```json
   [
       {
           "name": "HONEYCOMB_DATASET",
           "value": "otel-ai-chatbot-dev"
       },
       {
           "name": "OTEL_EXPORTER_OTLP_ENDPOINT",
           "value": "https://api.honeycomb.io"
       },
       {
           "name": "OTEL_SERVICE_NAME",
           "value": "otel-ai-chatbot-backend"
       }
   ]
   ```

2. **Verify secrets are configured**:
   ```bash
   pulumi env run honeycomb-pulumi-workshop/ws -i -- aws ecs describe-task-definition \
     --task-definition $(pulumi stack output ecsTaskDefinitionArn | cut -d'/' -f2) \
     --query 'taskDefinition.containerDefinitions[0].secrets[?name==`HONEYCOMB_API_KEY` || name==`OTEL_EXPORTER_OTLP_HEADERS`]'
   ```

   Expected output:
   ```json
   [
       {
           "name": "HONEYCOMB_API_KEY",
           "valueFrom": "arn:aws:secretsmanager:...secret:HONEYCOMB_API_KEY::"
       },
       {
           "name": "OTEL_EXPORTER_OTLP_HEADERS",
           "valueFrom": "arn:aws:secretsmanager:...secret:OTEL_EXPORTER_OTLP_HEADERS::"
       }
   ]
   ```

::alert[**Configuration via Environment**: OpenTelemetry's SDK reads configuration from environment variables, making it cloud-native and easy to configure without code changes.]{type="info"}

## Step 4: Check Application Logs for Tracing Initialization

1. **View recent CloudWatch logs**:
   ```bash
   cd /workshop/ai-workshop/pulumi
   pulumi env run honeycomb-pulumi-workshop/ws -i -- aws logs tail \
     /aws/ecs/otel-ai-chatbot-logs \
     --since 10m \
     --filter-pattern "OpenTelemetry"
   ```

   Look for these log lines:
   ```
   [OpenTelemetry] âœ¨ Tracing and logging initialized
   [OpenTelemetry] ðŸ“Š Service: otel-ai-chatbot-backend
   [OpenTelemetry] ðŸŒ Environment: development
   [OpenTelemetry] ðŸ”— Endpoint: https://api.honeycomb.io
   [OpenTelemetry] ðŸ“ Logs will be sent to both CloudWatch and Honeycomb
   ```

   If you see these logs, OpenTelemetry is successfully initialized! âœ…

2. **If logs don't show initialization**, check for errors:
   ```bash
   pulumi env run honeycomb-pulumi-workshop/ws -i -- aws logs tail \
     /aws/ecs/otel-ai-chatbot-logs \
     --since 10m \
     --filter-pattern "error"
   ```

## Step 5: Generate Traffic to the Application

Now let's generate some requests to create traces.

1. **Get the application URL**:
   ```bash
   cd /workshop/ai-workshop/pulumi
   ALB_URL=$(pulumi stack output albUrl)
   echo "Application URL: $ALB_URL"
   ```

2. **Send a few chat requests**:
   ```bash
   for i in {1..3}; do
     echo "Request $i:"
     curl -X POST "${ALB_URL}/api/chat" \
       -H "Content-Type: application/json" \
       -d '{"message": "How do I instrument Express.js with OpenTelemetry?"}' \
       -w "\nHTTP Status: %{http_code}, Time: %{time_total}s\n\n"
     sleep 2
   done
   ```

   Expected output:
   ```
   Request 1:
   {"success":true,"response":"To instrument Express.js...","sources":[...]}
   HTTP Status: 200, Time: 2.341s

   Request 2:
   {"success":true,"response":"To instrument Express.js...","sources":[...]}
   HTTP Status: 200, Time: 1.876s
   ```

3. **Check health endpoint** (note: this is excluded from tracing):
   ```bash
   curl "${ALB_URL}/api/health"
   ```

   Expected:
   ```json
   {
     "status": "healthy",
     "timestamp": "2024-01-15T10:30:45.123Z",
     "services": {
       "llm": "bedrock",
       "vectorStore": "opensearch"
     }
   }
   ```

::alert[**Health Check Exclusion**: The `/api/health` endpoint is intentionally excluded from tracing (see `config/tracing.js:79-82`) to avoid noisy telemetry from load balancer health checks.]{type="info"}

## Step 6: Verify Traces in Honeycomb

1. **Login to Honeycomb**: https://ui.honeycomb.io

2. **Select your dataset**: `otel-ai-chatbot-dev` (or `otel-ai-chatbot-ws` depending on your stack)

3. **View Recent Traces**:
   - You should see traces appearing in real-time
   - Each trace represents one chat request

4. **Click on a trace to examine it**:

   You'll see the trace waterfall showing:
   ```
   POST /api/chat (root span)
   â”œâ”€ llm.bedrock.call (~1.8s)
   â”‚  â””â”€ AWS Bedrock InvokeModel (~1.7s)
   â”œâ”€ db.opensearch.query (~0.3s)
   â”‚  â””â”€ HTTPS POST OpenSearch (~0.2s)
   â””â”€ serialization (~0.05s)
   ```

5. **Examine span attributes**:

   Click on the `gen_ai.bedrock.chat` span to see attributes:
   - `gen_ai.system` = "aws.bedrock"
   - `gen_ai.operation.name` = "chat"
   - `gen_ai.request.model` = "anthropic.claude-3-5-sonnet-20240620-v1:0"
   - `gen_ai.usage.input_tokens` = 652
   - `gen_ai.usage.output_tokens` = 234
   - `gen_ai.response.finish_reasons` = ["stop"]
   - `gen_ai.response.id` = "msg_01ABC..."
   - `server.address` = "bedrock-runtime.us-east-1.amazonaws.com"

::alert[**Success!** You're now seeing production-grade observability data. The application was instrumented from day one, and you only had to configure where to send the data.]{type="success"}

## Step 7: Understand the GenAI Semantic Conventions

The LLM tracing follows OpenTelemetry's **standardized semantic conventions for GenAI** workloads. These conventions were recently finalized as part of OpenTelemetry's official specifications, representing a major milestone for standardizing AI observability across the industry.

| Attribute | Description | Example |
|-----------|-------------|---------|
| `gen_ai.system` | The GenAI system/provider | "bedrock", "openai", "anthropic" |
| `gen_ai.request.model` | Model identifier | "anthropic.claude-3-5-sonnet-20240620-v1:0" |
| `gen_ai.operation.name` | Type of GenAI operation | "completion", "embedding", "chat" |
| `gen_ai.usage.input_tokens` | Input tokens consumed | 652 |
| `gen_ai.usage.output_tokens` | Output tokens generated | 234 |
| `gen_ai.response.finish_reasons` | Completion reason | "stop", "length", "error" |
| `gen_ai.prompt.0.content` | First prompt content (sampling) | "You are a helpful assistant..." |
| `gen_ai.response.id` | Unique response identifier | "msg_01ABC..." |
| `error.type` | Error type if failed | "RateLimitError", "InvalidRequest" |

**Why these attributes matter:**
- **Cost tracking**: Token usage directly correlates to API costs ($3-15 per million tokens)
- **Performance monitoring**: Duration helps identify slow models or API issues
- **Error analysis**: Distinguish between rate limits, timeouts, and invalid requests
- **Model comparison**: Compare performance across different models and versions
- **Prompt engineering**: Analyze which prompts lead to better outcomes (when sampled)
- **User attribution**: Track which users/teams consume the most tokens

::alert[**OpenTelemetry Standard**: As of 2025, GenAI semantic conventions are now part of the official OpenTelemetry specification. This ensures consistent instrumentation across all observability vendors. See https://opentelemetry.io/docs/specs/semconv/gen-ai/ for complete details.]{type="success"}

### Best Practice: Capturing the Right Context

When instrumenting GenAI applications, Honeycomb recommends capturing:

1. **All errors** - Not just LLM API failures, but also:
   - Invalid or malformed responses that cause downstream errors
   - Timeout errors from slow responses
   - Rate limiting and quota exceeded errors

2. **User/team identifiers** - High-cardinality attributes like:
   - `user.id` - Track per-user behavior and costs
   - `team.id` - Analyze usage by team or organization
   - `session.id` - Correlate multiple requests in a conversation

3. **The actual prompt** (with sampling) - For debugging and optimization:
   - Use `gen_ai.prompt.0.content` to capture the system prompt
   - **Privacy consideration**: Sample at ~1% in production to protect sensitive user data
   - Always capture prompts for errors (100% sampling on failures) to aid debugging
   - Consider redacting PII from prompts before sending to observability platforms

4. **RAG context** - If using retrieval-augmented generation:
   - `rag.documents_retrieved` - Number of documents fetched
   - `rag.documents_used` - Number actually included in context
   - `rag.retrieval_latency_ms` - Time spent on vector search

5. **Function/tool calls** - For agentic workflows:
   - `gen_ai.tools_count` - Number of tools available
   - `gen_ai.tool_calls` - Which tools were invoked
   - Each tool call as a child span with its own attributes

::alert[**Privacy vs. Visibility**: While Honeycomb doesn't charge extra for high-cardinality attributes, consider sampling sensitive data like full prompts/responses primarily for **privacy protection**. Capture 100% of metadata (tokens, model, latency, user IDs) but sample full prompt/response content at 1-10% depending on your privacy requirements. Always implement PII redaction for production workloads.]{type="info"}

## Step 8: Explore Log Correlation

One powerful feature of OpenTelemetry is **log-trace correlation**. Winston logs are automatically correlated with traces.

1. **View logs with trace context**:
   ```bash
   cd /workshop/ai-workshop/pulumi
   pulumi env run honeycomb-pulumi-workshop/ws -i -- aws logs tail \
     /aws/ecs/otel-ai-chatbot-logs \
     --since 5m \
     --filter-pattern "POST /api/chat"
   ```

   You'll see logs like:
   ```json
   {
     "message": "Incoming request: POST /api/chat",
     "level": "info",
     "trace_id": "7f8c9d2e1a3b4c5d6e7f8g9h0i1j2k3l",
     "span_id": "a1b2c3d4e5f6g7h8",
     "timestamp": "2024-01-15T10:30:45.123Z"
   }
   ```

2. **In Honeycomb**, click on a log event and you'll see:
   - **Trace ID**: Links directly to the associated trace
   - **Span ID**: Links to the specific span where the log occurred
   - **Service context**: Automatic enrichment with service name, version, environment

This correlation allows you to:
- Navigate from log â†’ trace
- Navigate from trace â†’ logs
- Understand the full context of any event

## Step 9: Best Practice - Deployment Markers (Optional)

Honeycomb supports **markers** to annotate your telemetry data with deployment events. This helps correlate performance changes with code deployments.

To create a marker when deploying:

```bash
# After a successful deployment
HONEYCOMB_API_KEY="your-api-key"
DATASET="otel-ai-chatbot-dev"

curl https://api.honeycomb.io/1/markers/$DATASET \
  -X POST \
  -H "X-Honeycomb-Team: $HONEYCOMB_API_KEY" \
  -d '{
    "message": "Deployed v1.2.3 - Improved LLM instrumentation",
    "type": "deploy",
    "url": "https://github.com/your-repo/commit/abc123"
  }'
```

Markers appear as vertical lines in your Honeycomb graphs, making it easy to see if latency increased or errors spiked after a deployment.

::alert[**CI/CD Integration**: Add marker creation to your CI/CD pipeline so every deployment is automatically annotated in Honeycomb. This is invaluable for incident response and performance regression analysis.]{type="info"}

## Step 10: Sampling Strategies for Production

While the workshop captures 100% of traces for demonstration purposes, **production deployments require sampling** to balance cost, privacy, and observability value.

### Understanding Sampling Trade-offs

**Why sample?**
- **Privacy Protection**: Avoid capturing sensitive user data in every trace
- **Cost Management**: Reduce data volume sent to observability backend
- **Performance**: Minimize instrumentation overhead in high-throughput services
- **Compliance**: Meet data retention and privacy regulations (GDPR, CCPA)

**What to always capture (100%):**
- âœ… Structured metadata: tokens, model, latency, user IDs, error types
- âœ… All errors and exceptions (critical for debugging)
- âœ… High-value transactions (user signups, purchases)
- âœ… Trace context for distributed tracing

**What to sample (1-10%):**
- ðŸ”’ Full prompt content (`gen_ai.prompt.*.content`)
- ðŸ”’ Full response content (`gen_ai.completion.*.content`)
- ðŸ”’ PII-containing fields
- ðŸ“Š Routine successful operations in high-traffic endpoints

### Sampling Approaches

#### Option 1: Head-Based Sampling (Simple)

Decides at trace start whether to sample. Good for development and testing.

```javascript
// config/tracing.js
import { TraceIdRatioBasedSampler } from '@opentelemetry/sdk-trace-base';

const sdk = new NodeSDK({
  sampler: new TraceIdRatioBasedSampler(0.1), // Sample 10% of traces
  // ... other config
});
```

**Pros**: Simple, deterministic, low overhead
**Cons**: Might miss important error traces if unlucky

#### Option 2: Tail-Based Sampling (Production-Recommended)

Examines complete trace before deciding to sample. Captures ALL errors while sampling successes.

```javascript
// Requires OpenTelemetry Collector
// collector-config.yaml
processors:
  tail_sampling:
    decision_wait: 10s
    policies:
      # Always sample errors
      - name: errors-policy
        type: status_code
        status_code: {status_codes: [ERROR]}

      # Always sample slow requests
      - name: slow-traces-policy
        type: latency
        latency: {threshold_ms: 1000}

      # Always sample on specific attributes
      - name: high-value-users
        type: attribute
        attribute: {key: user.tier, values: [premium, enterprise]}

      # Sample 1% of everything else
      - name: probabilistic-policy
        type: probabilistic
        probabilistic: {sampling_percentage: 1}
```

**Pros**: Intelligent sampling, captures all errors, production-grade
**Cons**: Requires Collector, more complex setup

#### Option 3: Content Sampling (Privacy-Focused)

Capture 100% of traces but sample sensitive content within spans.

```javascript
// server/utils/llmTracing.js
const CAPTURE_CONTENT = process.env.OTEL_GENAI_CAPTURE_CONTENT === 'true';
const CONTENT_SAMPLE_RATE = parseFloat(process.env.OTEL_GENAI_CONTENT_SAMPLE_RATE || '0.01');

export async function traceLLMCall(providerName, modelName, systemPrompt, userPrompt, fn, metadata = {}) {
  return tracer.startActiveSpan(
    `gen_ai.${providerName}.chat`,
    { /* ... attributes ... */ },
    async (span) => {
      try {
        const result = await fn();

        // Always capture metadata (100%)
        span.setAttributes({
          'gen_ai.usage.input_tokens': usage.promptTokens,
          'gen_ai.usage.output_tokens': usage.completionTokens,
          'gen_ai.response.id': result.id,
          'gen_ai.response.finish_reasons': [result.stop_reason]
        });

        // Conditionally capture content (1% sample rate)
        if (CAPTURE_CONTENT && (Math.random() < CONTENT_SAMPLE_RATE || span.isErrored())) {
          // Always capture on errors, otherwise sample at 1%
          span.setAttributes({
            'gen_ai.prompt.0.content': redactPII(systemPrompt),
            'gen_ai.prompt.1.content': redactPII(userPrompt),
            'gen_ai.completion.0.content': redactPII(result.content)
          });
        }

        return result;
      } catch (error) {
        // ALWAYS capture prompts on errors for debugging
        if (CAPTURE_CONTENT) {
          span.setAttributes({
            'gen_ai.prompt.0.content': redactPII(systemPrompt),
            'gen_ai.prompt.1.content': redactPII(userPrompt)
          });
        }
        throw error;
      }
    }
  );
}

// Simple PII redaction example
function redactPII(text) {
  return text
    .replace(/\b\d{3}-\d{2}-\d{4}\b/g, '[SSN]')           // SSN
    .replace(/\b[\w\.-]+@[\w\.-]+\.\w+\b/g, '[EMAIL]')    // Email
    .replace(/\b\d{16}\b/g, '[CARD]');                     // Credit card
}
```

**Pros**: Fine-grained control, privacy-first, captures all metadata
**Cons**: Requires code changes, manual PII redaction implementation

### Recommended Strategy for This Workshop

**Development/Testing:**
- âœ… Use head-based sampling at 100% (current implementation)
- âœ… No content capture by default (privacy-safe for workshop)

**Production:**
- âœ… Use tail-based sampling with OpenTelemetry Collector
- âœ… Capture 100% of errors and slow traces
- âœ… Sample 1-5% of successful traces
- âœ… Implement PII redaction for any captured content
- âœ… Use separate datasets for dev (100%) vs. prod (sampled)

::alert[**Workshop Note**: The current implementation captures 100% of traces with no prompt/response content. This is appropriate for the workshop. In production, implement tail-based sampling via OpenTelemetry Collector for optimal cost and privacy balance.]{type="success"}

## Module Summary

Congratulations! You've successfully:

âœ… Explored the existing OpenTelemetry instrumentation in the application
âœ… Understood auto-instrumentation for Express, HTTP, and AWS SDK
âœ… Reviewed custom GenAI tracing with v1.0 semantic conventions
âœ… Learned about high-cardinality attributes and why they matter
âœ… Verified Honeycomb configuration in the ECS task
âœ… Generated traffic and confirmed traces are flowing to Honeycomb
âœ… Examined trace structure and GenAI-specific attributes
âœ… Understood log-trace correlation
âœ… Learned deployment markers for correlating changes with performance
âœ… Explored sampling strategies for production deployments

## Key Takeaways

### 1. Observability-Driven Development Culture

Just like you wouldn't deploy without logging or error handling, you shouldn't deploy without observability. Build it in from day one. At Honeycomb, engineers add telemetry as a **core part** of every feature and bug fix, creating a culture where observability is everyone's responsibility.

### 2. High-Cardinality is a Feature, Not a Bug

Traditional monitoring tools force you to aggregate away high-cardinality data (like user IDs, request IDs, model versions). Honeycomb **embraces** high-cardinality, allowing you to ask questions like "Show me all requests from user X where model Y exceeded Z tokens"â€”without pre-defining these queries.

### 3. Auto-Instrumentation Does the Heavy Lifting

OpenTelemetry's auto-instrumentation captures 80% of telemetry with zero code changes. Custom instrumentation fills the remaining 20% for business-specific metrics like token usage, RAG performance, and agentic workflows.

### 4. Configuration is Declarative

Environment variables control where telemetry goes, making it easy to send data to different backends without code changes (Honeycomb today, Jaeger tomorrow). This is the OpenTelemetry promise: **vendor-neutral instrumentation**.

### 5. GenAI Workloads Need Specialized Observability

Token usage, model selection, and prompt engineering directly impact cost and performance. The OpenTelemetry GenAI semantic conventions (now standardized) ensure consistent instrumentation across providers, making it easy to compare Bedrock vs OpenAI vs Anthropic direct.

### 6. Balance Privacy and Visibility with Sampling

Capture 100% of structured metadata (tokens, model, latency, user ID), but sample sensitive unstructured data like full prompts/responses at 1-10% primarily for **privacy protection**. Always capture 100% on errors for debugging, ensuring PII is redacted first.

### 7. Context Propagation is Automatic

OpenTelemetry automatically propagates trace context across service boundaries (HTTP calls, async operations, AWS SDK calls), giving you end-to-end visibility from API gateway â†’ application â†’ Bedrock â†’ OpenSearch without manual plumbing.

## What's Next?

In **Module 3**, you'll use Honeycomb to analyze the telemetry data you're now collecting, identify performance bottlenecks, and discover resource constraints in the infrastructure.

---

## Additional Resources

- [OpenTelemetry Node.js Documentation](https://opentelemetry.io/docs/instrumentation/js/getting-started/nodejs/)
- [OpenTelemetry Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/)
- [OpenTelemetry GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/)
- [Honeycomb OpenTelemetry Setup Guide](https://docs.honeycomb.io/getting-data-in/opentelemetry/)
- [AWS Distro for OpenTelemetry](https://aws-otel.github.io/)
